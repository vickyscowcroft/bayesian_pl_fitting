{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import healpy as hp\n",
    "from esutil.coords import eq2gal, eq2ec ### esutil needs pip install\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from numpy.lib.recfunctions import append_fields\n",
    "from dust import getval\n",
    "from time import time\n",
    "import ctypes\n",
    "import emcee_helpers ### required pip install tables - can use astropy alternative? pytables?\n",
    "import emcee\n",
    "from emcee.interruptible_pool import InterruptiblePool\n",
    "from multiprocessing import cpu_count\n",
    "from scipy.stats import norm\n",
    "from subprocess import getoutput ## was commands in python2\n",
    "from joblib import Parallel, delayed\n",
    "import pickle ## was cPickle in python2\n",
    "import gzip\n",
    "from emcee.utils import MPIPool\n",
    "import sys\n",
    "import extcurve_s16\n",
    "from scipy.optimize import fminbound\n",
    "import acor\n",
    "import esutil\n",
    "from scipy.integrate import quad\n",
    "import matplotlib.pyplot as mp\n",
    "import pandas as pd\n",
    "from astroquery.irsa_dust import IrsaDust\n",
    "\n",
    "\n",
    "### requirements all satisfied/load in python 3.5 now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## setting up working directory\n",
    "work_dir = './'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the shared library\n",
    "_lib = ctypes.CDLL('./likelihood_function_quad.so')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of CPUs to use\n",
    "nproc = 4 \n",
    "### set to 4 for VS desktop - increase this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the integration function\n",
    "integration_function = _lib.integrate_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prototype the integrate_likelihood function\n",
    "integration_function.argtypes = (ctypes.c_double, ctypes.c_double, ctypes.POINTER(ctypes.c_double), ctypes.c_double)\n",
    "# the function returns a double\n",
    "integration_function.restype = ctypes.c_double\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_ext_coeffs(x):\n",
    "    \"\"\"Calculates the extinction coefficient for all bandpasses using the extinction curve of Schlafly et al. (2016)\"\"\"\n",
    "    # R(V) = 3.3 + 9.1*x (Section 5.3 of Schlafly et al. 2016)\n",
    "    # 1-sigma uncertainty in x is 0.02 --> 1-sigma uncertainty in R(V) is 0.18 mag\n",
    "    # will want to change these bandpasses to correspond to what we have in the Spitzer/MBS dataset\n",
    "    bands = [\"U\", \"B\", \"hipp\", \"V\", \"R\", \"I\", \"z\", \"J\", \"H\", \"K\", \"W1\", \"W2\", \"W3\"]\n",
    "    band_wavelengths = {\"U\":0.3663,\n",
    "                        \"B\":0.4361,\n",
    "                        \"hipp\":0.5170,\n",
    "                        \"V\":0.5448,\n",
    "                        \"R\":0.6407,\n",
    "                        \"I\":0.7980,\n",
    "                        \"z\":0.8896,\n",
    "                        \"J\":1.22,\n",
    "                        \"H\":1.63,\n",
    "                        \"K\":2.19,\n",
    "                        \"W1\":3.4,\n",
    "                        \"W2\":4.6,\n",
    "                        \"W3\":12.0\n",
    "                       }\n",
    "    ec = extcurve_s16.extcurve(x) # mean extinction curve\n",
    "    Egr = ec(4876.7) - ec(6200.1)\n",
    "    return np.array([ec(band_wavelengths[band]*10000.)/Egr for band in bands])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_unc_in_ext_coeff(Nsamples):\n",
    "    \"\"\"Calculates the uncertainty in each extinction coefficient given the uncertainty in x parameter (Section 5.3 of Schlafly et al. 2016)\"\"\"\n",
    "    arr = np.zeros((Nsamples, 13))\n",
    "    for i, x in enumerate(0.02*np.random.randn(Nsamples)):\n",
    "        arr[i, :] = calculate_ext_coeffs(x)\n",
    "    return np.std(arr, ddof=1, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to change the load_data function to read the dambis data file from a fits table directly into a pandas data frame. \n",
    "\n",
    "http://docs.astropy.org/en/stable/io/fits/\n",
    "\n",
    "https://stackoverflow.com/questions/40111872/construct-pandas-dataframe-from-a-fits-file\n",
    "\n",
    "Astroquery IRSA extinctions: https://github.com/astropy/astroquery/blob/master/docs/irsa/irsa_dust.rst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(P_ref=0.52854, FeH_ref=-1.4):\n",
    "    \n",
    "    dat = Table.read('Dambis_2013_Table2.fits', format='fits')\n",
    "    dambis_df = dat.to_pandas()    \n",
    "    kb_df = pd.read_csv('rrl_fit_table1.txt', delim_whitespace=True, header=0)\n",
    "    \n",
    "    ## Remove spaces in dambis names, match tables on names\n",
    "    ## adding id_compare column to each table\n",
    "    dambis_df['id_compare'] = [*map(str.lower, dambis_df.Name)] ### [*map(...)] syntax required for python3\n",
    "    dambis_df['id_compare'] = dambis_df['id_compare'].replace(regex=True, to_replace=r' ',value='')\n",
    "\n",
    "    kb_df['id_compare'] = [*map(str.lower, kb_df.name)]\n",
    "    kb_df['id_compare'] = kb_df['id_compare'].replace(regex=True, to_replace=r' ',value='')\n",
    "\n",
    "    merged_df = dambis_df.merge(kb_df, on='id_compare')\n",
    "    \n",
    "    ## Brani adds the Galactic coords here to grab the extinction. Not going to do that. \n",
    "    ## Calling IRSA astroquery to grab a list of extinctions instead. \n",
    "    \n",
    "    return(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: UnitsWarning: '[Sun]' did not parse as fits unit: Invalid character at col 0 [astropy.units.core]\n"
     ]
    }
   ],
   "source": [
    "merged_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109, 77)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data[merged_data['RRt']=='AB'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the data files to pandas tables and matching them this way gives a sample of 109 RRab. Consistent with Brani's statement in the paper of approx 100 RRab stars, but an easier method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-f0216dfc0766>, line 54)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-f0216dfc0766>\"\u001b[0;36m, line \u001b[0;32m54\u001b[0m\n\u001b[0;31m    tbl = tbl[tbl['sigma_par'] <> 0]\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def load_data(P_ref=0.52854, FeH_ref=-1.4):\n",
    "\n",
    "    # load Table 2 of Dambis et al. (2013)\n",
    "    dambis = getdata('Dambis_2013_Table2.fits')\n",
    "    ## observational data - equivalent of what we have in the MBS paper.\n",
    "\n",
    "    # load Table 1 of Klein & Bloom (2013) \n",
    "    # Mid-IR data, 1st guesses on distances. Info about blazhko classificaiton.\n",
    "    tbl = np.genfromtxt(\n",
    "        'rrl_fit_table1.txt',\n",
    "        names='name, type, blazhko, period, log10(P_f/P_0), FeH, prior_mu, prior_mu_err, SF_ebv, SF_ebv_err, m_U_obs, m_U_obs_err, m_B_obs, m_B_obs_err, m_hipp_obs, m_hipp_obs_err, m_V_obs, m_V_obs_err, m_R_obs, m_R_obs_err, m_I_obs, m_I_obs_err, m_z_obs, m_z_obs_err, m_J_obs, m_J_obs_err, m_H_obs, m_H_obs_err, m_K_obs, m_K_obs_err, m_W1_obs, m_W1_obs_err, m_W2_obs, m_W2_obs_err, m_W3_obs, m_W3_obs_err, post_mu, post_mu_err',\n",
    "        dtype='|S10, |S5, bool, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8, f8',\n",
    "        skip_header=1\n",
    "    )\n",
    "    tbl = append_fields(tbl, names=('ra', 'dec', 'gl', 'gb', 'EBV', 'FeHErr', 'par_obs', 'sigma_par'), data=(np.zeros(tbl.size), np.zeros(tbl.size), np.zeros(tbl.size), np.zeros(tbl.size), np.zeros(tbl.size), np.zeros(tbl.size) + 0.15, np.zeros(tbl.size), np.zeros(tbl.size)), usemask=False, asrecarray=False)\n",
    "    ### adding null columns for RA, Dec, gal coords, extinction, Fe/H unc and other things to kb table. Don't need to do this. Do it later with joins. \n",
    "    # uncertainty in [Fe/H] is 0.15 dex (Note 8 in Table 1 of Fernley et al. 1998)\n",
    "\n",
    "    # remove spaces from Table 2 names\n",
    "    for i, name in enumerate(dambis['Name']):\n",
    "        dambis['Name'][i] = name.replace(' ', '')\n",
    "        if dambis['Name'][i][0:2] == 'V0':\n",
    "            dambis['Name'][i] = name.replace('V0', 'V').replace(' ', '')\n",
    "\n",
    "    # match Table 2 and Table 1 to get positions of RR Lyrae stars\n",
    "    for i, name in enumerate(tbl['name']):\n",
    "        ind = np.where(name == dambis['Name'])[0]\n",
    "        if ind.size > 0:\n",
    "            tbl['ra'][i] = dambis['RAJ2000'][ind[0]]\n",
    "            tbl['dec'][i] = dambis['DEJ2000'][ind[0]]\n",
    "        else:\n",
    "            print(\"Object %s has no data!\" % name)\n",
    "\n",
    "    # add galactic coordinates\n",
    "    gl, gb = eq2gal(tbl['ra'], tbl['dec'])\n",
    "    tbl['gl'] = gl\n",
    "    tbl['gb'] = gb\n",
    "\n",
    "    # add extinction\n",
    "    tbl['EBV'] = getval(tbl['gl'], tbl['gb'])\n",
    "\n",
    "    # add TGAS parallaxes\n",
    "    tgas = np.genfromtxt('tgas_match.txt', names=True)\n",
    "    # back out TGAS uncertainty corrections\n",
    "    tgas['parallax_error'] = np.sqrt(tgas['parallax_error']**2 - 0.2**2)/1.4\n",
    "\n",
    "    h=esutil.htm.HTM(10)\n",
    "    m1, m2, d12 = h.match(tbl['ra'], tbl['dec'], tgas['ra'], tgas['dec'], 5/3600., maxmatch=1)\n",
    "    tbl['par_obs'][m1] = tgas['parallax'][m2]/1000.\n",
    "    tbl['sigma_par'][m1] = tgas['parallax_error'][m2]/1000.\n",
    "    tbl['ra'][m1] = tgas['ra'][m2]\n",
    "    tbl['dec'][m1] = tgas['dec'][m2]\n",
    "\n",
    "    # eliminate objects without TGAS parallaxes\n",
    "    tbl = tbl[tbl['sigma_par'] <> 0]\n",
    "    \n",
    "    \n",
    "    ##################################\n",
    "    \n",
    "    ## below here is defining PLs, priors etc. Should split function here?\n",
    "    \n",
    "\n",
    "    # define the PLRs (Table 2 of Klein & Bloom 2013)\n",
    "    \n",
    "    # Is there a better way I can do this? Don't like the K&B PLs. Marconi? Neeley?\n",
    "    \n",
    "    klein2014_table2 = np.genfromtxt('%s/Klein_et_al_2014_Table2.csv' % work_dir, delimiter=',',names=True, dtype='|S10,f8,f8,f8,f8,f8,f8,f8,f8')\n",
    "\n",
    "    bands = klein2014_table2['band']\n",
    "    alphas = klein2014_table2['alpha']\n",
    "    betas = klein2014_table2['beta']\n",
    "    gammas = np.array([0.5, 0.4, 0.3, 0.3, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.1, 0.1, 0.1])\n",
    "    #alphas[9] = -2.326 # K-band slope from Braga et al. (2015)\n",
    "    alphas[9] = -2.38 # +- 0.04, from Sollima et al. (2006)\n",
    "    gammas[9] = 0.09 # +- 0.14, from Sollima et al. (2006)\n",
    "    #ax = (alphas[9]+0.04*np.random.randn(1000))*np.log10(P_ref) + (gammas[9] + 0.14*np.random.randn(1000))*FeH_ref -1.04 + 0.13*np.random.randn(1000)\n",
    "    betas[9] = -0.50 # np.mean(ax)\n",
    "    alphas[10] = -2.381 # +- 0.097, from Dambis et al. (2014)\n",
    "    gammas[10] = 0.106 # +- 0.023, from Dambis et al. (2014)\n",
    "    #ax = (alphas[10]+0.097*np.random.randn(1000))*np.log10(P_ref) + (gammas[10] + 0.023*np.random.randn(1000))*FeH_ref - 1.135 + 0.077*np.random.randn(1000)\n",
    "    betas[10] = -0.62\n",
    "    alphas[11] = -2.269 # +- 0.127, from Dambis et al. (2014)\n",
    "    gammas[11] = 0.117 # +- 0.023, from Dambis et al. (2014)\n",
    "    #ax = (alphas[11]+0.127*np.random.randn(1000))*np.log10(P_ref) + (gammas[11] + 0.023*np.random.randn(1000))*FeH_ref - 1.088 + 0.077*np.random.randn(1000)\n",
    "    betas[11] = -0.62\n",
    "\n",
    "    # set scatter in all PLRs to 0.001 mag to measure the uncertainty in recovered parameters\n",
    "    sigma_PLRs = np.zeros(bands.size) + 0.001\n",
    "    #sigma_PLRs = klein2014_table2['sig_intrinsic']\n",
    "\n",
    "    #ext_coeffs = np.array([4.334, 3.626, 3.0, 2.742, 2.169, 1.505, 1.263, 0.709, 0.449, 0.302, 0.2, 0.1, 0.05])\n",
    "    ext_coeffs = calculate_ext_coeffs(0)*0.96710433795664874 # A_band = C_band * E(g-r), following Schlafly et al. (2016); multiply by 0.96710433795664874 to get A_band = C_band * E(B-V)\n",
    "    # uncertainty in extinction coefficients assuming sigma(x) = 0.02 or sigma(RV) = 0.18 and Schlafly et al. (2016) extinction curve\n",
    "    ext_coeffs_unc = np.array([0.14, 0.14, 0.14, 0.14, 0.14, 0.13, 0.12, 0.07, 0.04, 0.03, 0.01, 0.001, 0.05])\n",
    "\n",
    "    obs_mags = np.zeros((tbl.size, bands.size))\n",
    "    obs_magErrs = np.zeros((tbl.size, bands.size))\n",
    "\n",
    "    Nstars = obs_mags.shape[0]\n",
    "    Nbands = obs_mags.shape[1]\n",
    "\n",
    "    for b, ext_coeff in enumerate(ext_coeffs):\n",
    "        obs_mags[:, b] = tbl['m_%s_obs' % bands[b]]\n",
    "        obs_magErrs[:, b] = tbl['m_%s_obs_err' % bands[b]]\n",
    "\n",
    "    log10P_fP_0 = tbl['log10P_fP_0']\n",
    "    EBV = tbl['EBV']\n",
    "\n",
    "    # RIJHK: Table 2 of Braga et al. (2015)\n",
    "    # alphas: R = -0.847 +- 0.177, I = -1.137 +- 0.144, J = -1.793 +- 0.109, H = -2.408 +- 0.082, K = -2.326 +- 0.074\n",
    "    # W1, W2, W3: Eqs. 2-7 of Klein et al. (2014)\n",
    "    # DM of M4 is 11.4 (Neeley et al. 2015)\n",
    "\n",
    "    # Gaussian priors on betas, alphas, gammas, and sigma_PLRs from Table 2 of Klein et al. (2013)\n",
    "    alpha_mean = klein2014_table2['alpha']\n",
    "    alpha_sig = klein2014_table2['alpha_sig']\n",
    "    #alpha_mean[9] = -2.326 # from Braga et al. (2015)\n",
    "    #alpha_sig[9] = 0.074 # from Braga et al. (2015)\n",
    "    alpha_mean[9] = -2.38 # from Sollima et al. (2006)\n",
    "    alpha_sig[9] = 0.04 # from Sollima et al. (2006)\n",
    "    alpha_mean[10] = -2.381 # from Dambis et al. (2014)\n",
    "    alpha_sig[10] = 0.097 # from Dambis et al. (2014)\n",
    "    alpha_mean[11] = -2.269 # from Dambis et al. (2014)\n",
    "    alpha_sig[11] = 0.127 # from Dambis et al. (2014)\n",
    "    beta_mean = klein2014_table2['beta']\n",
    "    beta_sig = klein2014_table2['beta_sig']\n",
    "    beta_mean[9] = -0.50\n",
    "    beta_sig[9] = 0.23\n",
    "    beta_mean[10] = -0.62\n",
    "    beta_sig[10] = 0.09\n",
    "    beta_mean[11] = -0.62\n",
    "    beta_sig[11] = 0.09\n",
    "    gamma_mean = gammas\n",
    "    gamma_sig = gammas*0 + 0.14\n",
    "    gamma_mean[10] = 0.106 # from Dambis et al. (2014)\n",
    "    gamma_sig[10] = 0.023 # from Dambis et al. (2014)\n",
    "    gamma_mean[11] = 0.117 # from Dambis et al. (2014)\n",
    "    gamma_sig[11] = 0.023 # from Dambis et al. (2014)\n",
    "    #sigma_PLR_mean = klein2014_table2['sig_intrinsic']\n",
    "    #sigma_PLR_sig = klein2014_table2['sig_intrinsic_sig']\n",
    "    sigma_PLR_mean = np.zeros(bands.size) + 0.001\n",
    "    sigma_PLR_sig = np.zeros(bands.size) + 0.0001\n",
    "\n",
    "    # hard limits on PLR parameters, mean +- 7*sigma\n",
    "    alphas_lower_limit = []\n",
    "    alphas_upper_limit = []\n",
    "    for alpha, sig in zip(alpha_mean, alpha_sig):\n",
    "        alphas_lower_limit.append(alpha - 7*sig)\n",
    "        alphas_upper_limit.append(alpha + 7*sig)\n",
    "\n",
    "    alphas_lower_limit = np.array(alphas_lower_limit)\n",
    "    alphas_upper_limit = np.array(alphas_upper_limit)\n",
    "\n",
    "    betas_lower_limit = []\n",
    "    betas_upper_limit = []\n",
    "    for beta, sig in zip(beta_mean, beta_sig):\n",
    "        betas_lower_limit.append(beta - 7*sig)\n",
    "        betas_upper_limit.append(beta + 7*sig)\n",
    "\n",
    "    betas_lower_limit = np.array(betas_lower_limit)\n",
    "    betas_upper_limit = np.array(betas_upper_limit)\n",
    "\n",
    "    # use broad priors for the W1 and W2 bands\n",
    "    alphas_upper_limit[10:12] = 2\n",
    "    alphas_lower_limit[10:12] = -5\n",
    "    betas_upper_limit[10:12] = 0\n",
    "    betas_lower_limit[10:12] = -4\n",
    "\n",
    "    # use broad priors for the Hipparcos band\n",
    "    alphas_upper_limit[2] = 2\n",
    "    alphas_lower_limit[2] = -5\n",
    "    betas_upper_limit[2] = 2\n",
    "    betas_lower_limit[2] = -2\n",
    "\n",
    "    # this prior is not used right now\n",
    "    sig_PLR_lower_limit = []\n",
    "    sig_PLR_upper_limit = []\n",
    "    for sig_PLR, sig in zip(sigma_PLR_mean, sigma_PLR_sig):\n",
    "        sig_PLR_lower_limit.append(sig_PLR - 5*sig)\n",
    "        sig_PLR_upper_limit.append(sig_PLR + 5*sig)\n",
    "\n",
    "    sig_PLR_lower_limit = np.array(sig_PLR_lower_limit)\n",
    "    sig_PLR_lower_limit = np.where(sig_PLR_lower_limit <= 0, 0.00001, sig_PLR_lower_limit)\n",
    "    sig_PLR_upper_limit = np.array(sig_PLR_upper_limit)\n",
    "\n",
    "    # dump all important data\n",
    "    #### now we're into tables. Can I do this with pandas? \n",
    "    dat = {}\n",
    "    dat['scenario'] = 'real'\n",
    "    dat['name'] = tbl['name']\n",
    "    dat['ra'] = tbl['ra']\n",
    "    dat['dec'] = tbl['dec']\n",
    "    dat['P_ref'] = P_ref\n",
    "    dat['type'] = tbl['type']\n",
    "    dat['blazhko'] = tbl['blazhko']\n",
    "    dat['FeH_ref'] = FeH_ref\n",
    "    dat['obs_mags'] = obs_mags\n",
    "    dat['obs_magErrs'] = obs_magErrs\n",
    "    dat['par_obs'] = tbl['par_obs']\n",
    "    dat['sigma_par'] = tbl['sigma_par']\n",
    "    dat['log10P_fP_0'] = log10P_fP_0\n",
    "    dat['EBV'] = EBV\n",
    "    dat['FeH'] = tbl['FeH']\n",
    "    dat['FeHErr'] = tbl['FeHErr']\n",
    "    dat['alphas_lower_limit'] = alphas_lower_limit\n",
    "    dat['alphas_upper_limit'] = alphas_upper_limit\n",
    "    dat['betas_lower_limit'] = betas_lower_limit\n",
    "    dat['betas_upper_limit'] = betas_upper_limit\n",
    "    dat['sig_PLR_lower_limit'] = sig_PLR_lower_limit\n",
    "    dat['sig_PLR_upper_limit'] = sig_PLR_upper_limit\n",
    "    dat['alpha_mean'] = alpha_mean\n",
    "    dat['alpha_sig'] = alpha_sig\n",
    "    dat['beta_mean'] = beta_mean\n",
    "    dat['beta_sig'] = beta_sig\n",
    "    dat['gamma_mean'] = gamma_mean\n",
    "    dat['gamma_sig'] = gamma_sig\n",
    "    dat['sigma_PLR_mean'] = sigma_PLR_mean\n",
    "    dat['sigma_PLR_sig'] = sigma_PLR_sig\n",
    "    dat['ext_coeffs'] = ext_coeffs\n",
    "    dat['ext_coeffs_unc'] = ext_coeffs_unc\n",
    "    with gzip.open('TGAS_data.pklz', 'wb') as f:\n",
    "        cPickle.dump(dat, f)\n",
    "\n",
    "    return dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnprob(theta, dat, bands_to_process=[11], rel_tolerance=1e-09, FeH_ref=-1.4, P_ref=0.52854):\n",
    "    Nbands = len(bands_to_process)\n",
    "    betas = theta[0:Nbands]\n",
    "    alphas = theta[Nbands:2*Nbands]\n",
    "    ln_gammas = theta[2*Nbands:3*Nbands]\n",
    "    ln_sigma_PLRs = theta[3*Nbands:4*Nbands]\n",
    "    ln_f_par = theta[4*Nbands]\n",
    "    par_offset = theta[4*Nbands + 1]\n",
    "    ln_sigma_par_sys = theta[4*Nbands + 2]\n",
    "    ln_scale_length = theta[4*Nbands + 3]\n",
    "    if (any(alphas < dat['alphas_lower_limit'][bands_to_process]) | any(alphas > dat['alphas_upper_limit'][bands_to_process]) |\n",
    "        any(betas < dat['betas_lower_limit'][bands_to_process]) | any(betas > dat['betas_upper_limit'][bands_to_process]) |\n",
    "        any(ln_gammas < -7) | any(ln_gammas > 0) | any(ln_sigma_PLRs < -7) | any(ln_sigma_PLRs > -0.7) |\n",
    "        (ln_f_par < -7) | (ln_f_par > 2) |\n",
    "        (np.abs(par_offset) > 0.002) |\n",
    "        (ln_sigma_par_sys < np.log(0.001/1000.)) | (ln_sigma_par_sys > np.log(2./1000.)) |\n",
    "        (ln_scale_length < np.log(50)) | (ln_scale_length > np.log(5000))\n",
    "       ):\n",
    "        return -np.inf\n",
    "    gammas = np.exp(ln_gammas)\n",
    "    sigma_PLRs = np.exp(ln_sigma_PLRs)\n",
    "    f_par = np.exp(ln_f_par)\n",
    "    sigma_par_sys = np.exp(ln_sigma_par_sys)\n",
    "    scale_length = np.exp(ln_scale_length)\n",
    "    lnL = 0\n",
    "    # loop over bands\n",
    "    for b, band in enumerate(bands_to_process):\n",
    "        # calculate the likelihood for a single star\n",
    "        for star in np.where(np.isfinite(dat['obs_mags'][:, band]))[0]:\n",
    "            # variance in the corrected parallax\n",
    "            var_par = (f_par*dat['sigma_par'][star])**2 + sigma_par_sys**2\n",
    "            # predicted absolute magnitude of this star assuming some PLZ parameters for this band\n",
    "            Mpred = betas[b] + alphas[b]*dat['log10P_fP_0'][star] + gammas[b]*(dat['FeH'][star] - FeH_ref)\n",
    "            # variance in the predicted absolute magnitude\n",
    "            var_Mpred = (gammas[b]*dat['FeHErr'][star])**2 + (alphas[b]*0.02*dat['log10P_fP_0'][star])**2\n",
    "            # dereddened magnitude\n",
    "            mag_dereddened = dat['obs_mags'][star, band] - dat['ext_coeffs'][band]*dat['EBV'][star]\n",
    "            # variance in the dereddened magnitude\n",
    "            var_mag = dat['obs_magErrs'][star, band]**2 + (dat['ext_coeffs'][band]*(dat['ext_coeffs_unc'][band]+0.1)*dat['EBV'][star])**2 + sigma_PLRs[b]**2\n",
    "            # \"observed/predicted\" distance modulus\n",
    "            DM = mag_dereddened - Mpred\n",
    "            # variance in DM\n",
    "            var_DM = var_mag + var_Mpred\n",
    "            # find the MAP distance\n",
    "            distance_limits = (200., 2700.)\n",
    "            # determine optimal integration limits for distance d\n",
    "            d_MAP = 10*10**(0.2*(mag_dereddened - Mpred))\n",
    "            d_min = 10*10**(0.2*(5*np.log10(d_MAP) - 5 - np.sqrt(var_mag)))\n",
    "            d_max = 10*10**(0.2*(5*np.log10(d_MAP) - 5 + np.sqrt(var_mag)))\n",
    "            sigma_d = np.max([d_MAP-d_min, d_max-d_MAP])\n",
    "            d_min = np.max([distance_limits[0], d_MAP - 5*sigma_d])\n",
    "            d_max = np.min([distance_limits[1], d_MAP + 5*sigma_d])\n",
    "            integral = integration_function(ctypes.c_double(d_min), ctypes.c_double(d_max), (ctypes.c_double*6)(*[dat['par_obs'][star], var_par, DM, var_DM, par_offset, scale_length]), ctypes.c_double(rel_tolerance))\n",
    "            # likelihood for this star and band\n",
    "            integrated_likelihood = 1./np.sqrt((2*np.pi)**2*var_par*var_DM) * 1./(2*scale_length**3) * integral\n",
    "            if integrated_likelihood > 0:\n",
    "                lnL += np.log(integrated_likelihood)\n",
    "            else:\n",
    "                lnL += (-200)\n",
    "    lnprior = np.sum(np.log(gammas) + np.log(f_par))# + norm.logpdf(alphas, loc=-2.25, scale=0.01))\n",
    "    return lnL + lnprior - 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-4f8fa4ed32a7>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-4f8fa4ed32a7>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    print '%d stars in the sample.' % np.where(good)[0].size\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def run_emcee(dat, bands_to_process=[11], rel_tolerance=1e-09, debug=True):\n",
    "\n",
    "    Nbands = len(bands_to_process)\n",
    "\n",
    "    ndim = 4*Nbands + 4 # number of parameters in the model\n",
    "    nwalkers = 160 # number of MCMC walkers\n",
    "    nburn = 1000 # ignore this number of steps at the beginning\n",
    "    nsteps = 2500\n",
    "\n",
    "    theta0 = np.array(list(dat['beta_mean'][bands_to_process]) + list(dat['alpha_mean'][bands_to_process]) + list(np.log(dat['gamma_mean'][bands_to_process])) + list(np.log(dat['sigma_PLR_mean'][bands_to_process])) + [np.log(1.0)] + [0.0/1000.] + [np.log(0.17/1000.)] + [np.log(500.)])\n",
    "\n",
    "    # use Gaussian widths of priors to generate initial guesses for slope, intercept and sigma\n",
    "    starting_guesses = np.random.normal(theta0, [beta for beta in dat['beta_sig'][bands_to_process]]+[alpha for alpha in dat['alpha_sig'][bands_to_process]]+[gamma for gamma in dat['gamma_sig'][bands_to_process]]+[sig for sig in dat['sigma_PLR_sig'][bands_to_process]]+[0.2]+[0.1/1000.]+[0.2]+[0.2], (nwalkers, ndim))\n",
    "\n",
    "    # clip the guesses to appropriate ranges\n",
    "    starting_guesses = np.clip(starting_guesses, list(dat['betas_lower_limit'][bands_to_process])+list(dat['alphas_lower_limit'][bands_to_process])+list(np.zeros(Nbands)-7)+list(np.zeros(Nbands)-7)+[-7]+[-2./1000]+[np.log(0.001/1000.)]+[np.log(50)], list(dat['betas_upper_limit'][bands_to_process])+list(dat['alphas_upper_limit'][bands_to_process])+list(np.zeros(Nbands))+list(np.zeros(Nbands)-0.7)+[2]+[2./1000]+[np.log(2/1000.)]+[np.log(5000)])\n",
    "\n",
    "    # limit the sample to some subsample\n",
    "    good = np.abs(dat['sigma_par']/dat['par_obs']) < 10\n",
    "    #good = dat['FeH'] > -2.0\n",
    "    good = dat['type'] == 'RRab'\n",
    "    print '%d stars in the sample.' % np.where(good)[0].size\n",
    "\n",
    "    dat_good = dat.copy()\n",
    "    dat_good['log10P_fP_0'] = dat_good['log10P_fP_0'][good]\n",
    "    dat_good['par_obs'] = dat_good['par_obs'][good]\n",
    "    dat_good['sigma_par'] = dat_good['sigma_par'][good]\n",
    "    dat_good['obs_mags'] = dat_good['obs_mags'][good, :]\n",
    "    dat_good['obs_magErrs'] = dat_good['obs_magErrs'][good, :]\n",
    "    dat_good['FeH'] = dat_good['FeH'][good]\n",
    "    dat_good['FeHErr'] = dat_good['FeHErr'][good]\n",
    "    dat_good['EBV'] = dat_good['EBV'][good]\n",
    "\n",
    "    # check that we are not getting -np.inf for starting guesses\n",
    "    if debug:\n",
    "        print \"Log-likelihood for the initial guess = %.2f\" % lnprob(theta0, dat_good, bands_to_process=bands_to_process, rel_tolerance=rel_tolerance)\n",
    "    with Parallel(n_jobs=nproc) as parallel:\n",
    "        lnP = parallel(delayed(lnprob)(p, dat_good, bands_to_process=bands_to_process, rel_tolerance=rel_tolerance) for p in starting_guesses)\n",
    "    bad = np.where(~np.isfinite(lnP))[0]\n",
    "    if (bad.size > 0):\n",
    "        starting_guesses[bad] = theta0\n",
    "        if debug:\n",
    "            print \"Warning: Fixed %d bad initial guesses\" % bad.size\n",
    "\n",
    "    pool = InterruptiblePool(processes=nproc)\n",
    "\n",
    "    # initialize the sampler\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, pool=pool, args=[dat_good], kwargs={'rel_tolerance':rel_tolerance, 'bands_to_process':bands_to_process})\n",
    "\n",
    "    # run the chains\n",
    "    scenario = dat['scenario']\n",
    "    pos, prob, state = emcee_helpers.run_mcmc_with_pbar(sampler, starting_guesses, nsteps)\n",
    "\n",
    "    pool.terminate()\n",
    "\n",
    "    # check acceptance fraction and measure autocorrelation time\n",
    "    if debug:\n",
    "        print\n",
    "        print\n",
    "        print 'Minimum, mean, and maximum acceptance fraction: %.2f, %.2f, %.2f' % (np.min(sampler.acceptance_fraction), np.mean(sampler.acceptance_fraction), np.max(sampler.acceptance_fraction))\n",
    "        acors = []\n",
    "        for param in np.arange(ndim):\n",
    "            acors.append(acor.acor(np.mean(sampler.chain, axis=0)[:, param])[0])\n",
    "        acors = np.array(acors)\n",
    "        print \"Maximum autocorrelation time is %.2f\" % acors.max()\n",
    "\n",
    "    # check that the chains have converged\n",
    "    med = np.median(sampler._lnprob[:, -1])\n",
    "    rms = 0.741*(np.percentile(sampler._lnprob[:, -1], 75) - np.percentile(sampler._lnprob[:, -1], 25))\n",
    "    if debug:\n",
    "        plt.figure(1)\n",
    "        plt.clf()\n",
    "        plt.imshow(sampler.lnprobability, aspect=(1.*nsteps/nwalkers), vmin=np.percentile(sampler.lnprobability[:, -1], 5))\n",
    "        plt.xlabel('step')\n",
    "        plt.ylabel('walker')\n",
    "        plt.colorbar()\n",
    "        plt.savefig('%s/lnProb_walkers_%s_%.0e.png' % (work_dir, scenario, rel_tolerance))\n",
    "\n",
    "    # determine nburn by checking the convergence of chains\n",
    "    if debug:\n",
    "        param_num = -1\n",
    "        plt.clf()\n",
    "        plt.plot(sampler.chain[:, :, param_num].T)\n",
    "        plt.plot(np.median(sampler.chain[:, :, param_num].T, axis=1), 'k-', lw=3)\n",
    "        plt.xlabel(\"step\")\n",
    "        plt.ylabel('Parameter %d' % param_num)\n",
    "        plt.savefig('%s/chains_%s_%.0e.png' % (work_dir, scenario, rel_tolerance))\n",
    "\n",
    "    # dump burned chains\n",
    "    bands = np.array([\"U\", \"B\", \"hipp\", \"V\", \"R\", \"I\", \"z\", \"J\", \"H\", \"K\", \"W1\", \"W2\", \"W3\"])[bands_to_process]\n",
    "\n",
    "    # exponentiate some parameters\n",
    "    for b, band in enumerate(bands):\n",
    "        sampler.chain[:, :, b*4 + 2] = np.exp(sampler.chain[:, :, b*4 + 2])\n",
    "        sampler.chain[:, :, b*4 + 3] = np.exp(sampler.chain[:, :, b*4 + 3])\n",
    "\n",
    "    sampler.chain[:, :, -1] = np.exp(sampler.chain[:, :, -1])\n",
    "    sampler.chain[:, :, -2] = np.exp(sampler.chain[:, :, -2])\n",
    "    sampler.chain[:, :, -4] = np.exp(sampler.chain[:, :, -4])\n",
    "\n",
    "    param_list = ''\n",
    "    for param in ['beta', 'alpha', 'gamma', 'sigma']:\n",
    "        for band in bands:\n",
    "            param_list = param_list + param+'_'+band+','\n",
    "\n",
    "    param_list = param_list + 'f_par,par_offset,sigma_par_sys,scale_len'\n",
    "\n",
    "    # dump final chains\n",
    "    sampler = emcee_helpers.SamplerData(sampler, theta0)\n",
    "    sampler.attrs['theta_names'] = param_list.split(',')\n",
    "    sampler.writeto('%s/PLR_fit_%s_%.0e.h5' % (work_dir, scenario, rel_tolerance))\n",
    "\n",
    "    # eliminate bad chains using logprob\n",
    "    good_chains = sampler.lnprobability[:, -1] > np.percentile(sampler.lnprobability[:, -1], 5)\n",
    "    if debug:\n",
    "        print \"%d good chains (out of %d).\" % (np.where(good_chains)[0].size, nwalkers)\n",
    "\n",
    "    # use only good chains for parameter statistics\n",
    "    if debug:\n",
    "        sampler.chain = sampler.chain[good_chains, nburn:, :]\n",
    "        sampler.lnprobability = sampler.lnprobability[good_chains, nburn:]\n",
    "        # most probable values\n",
    "        print\n",
    "        print\n",
    "        print \"Most probable values:\"\n",
    "        max_prob = sampler.flatchain[np.argmax(sampler.flatlnprobability)]\n",
    "        for i, par in enumerate(sampler.attrs['theta_names']):\n",
    "            print '%s = %.5f' % (par, max_prob[i])\n",
    "        print \"------------------\"\n",
    "\n",
    "        # median, 16th and 84th percentile (1-sigma)\n",
    "        print\n",
    "        print \"Median, 16th and 84th percentile (1-sigma):\"\n",
    "        stats = np.percentile(sampler.flatchain, [16, 50, 84], axis=0)\n",
    "        for i, par in enumerate(sampler.attrs['theta_names']):\n",
    "            print '%s = %.5f  %.5f + %.5f' % (par, stats[1][i],\n",
    "                                              stats[0][i] - stats[1][i],\n",
    "                                              stats[2][i] - stats[1][i]\n",
    "                                             )\n",
    "\n",
    "    return sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (fit_PLRs.py, line 102)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/Users/vs522/Dropbox/Python/sesar_fitting_code/fit_PLRs.py\"\u001b[0;36m, line \u001b[0;32m102\u001b[0m\n\u001b[0;31m    print \"Object %s has no data!\" % name\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Usage: ./fit_PLRs\n",
    "    \"\"\"\n",
    "\n",
    "    import fit_PLRs\n",
    "    import os\n",
    "    import gzip\n",
    "    import cPickle\n",
    "    import sys\n",
    "    from commands import getoutput\n",
    "    from scipy import integrate\n",
    "    import ctypes\n",
    "\n",
    "    work_dir = '.'\n",
    "\n",
    "    dat = fit_PLRs.load_data()\n",
    "    #with gzip.open('%s/TGAS_data.pklz' % work_dir, 'rb') as f:\n",
    "    #    dat = cPickle.load(f)\n",
    "\n",
    "    sampler = fit_PLRs.run_emcee(dat, bands_to_process=[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
